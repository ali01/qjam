\documentclass[%
  %draft,
  %submission,
  %compressed,
  final,
  %
  %technote,
  %internal,
  %submitted,
  %inpress,
  %reprint,
  %
  %titlepage,
  notitlepage,
  %anonymous,
  narroweqnarray,
  inline,
  %twoside,
]{ieee}

\usepackage{ieeefig, url, enumerate}

\begin{document}

\title{Parallelizing Machine Learning Algorithms}

\author[SHORT NAMES]{
  \begin{tabular*}{0.75\textwidth}{@{\extracolsep{\fill}}cccc}
    Juan Batiz-Benet & Quinn Slack & Matt Sparks & Ali Yahya \\
    \multicolumn{4}{c}{
      \normalsize
      \url{{jbenet,sqs,msparks,alive}@cs.stanford.edu}}
  \end{tabular*}
}

\maketitle

\begin{abstract}
Qjam provides a portable framework for the rapid prototyping of machine
learning algorithms on a cluster of computers.
\end{abstract}

\section{Introduction}
The sheer number of computations that Machine Learning algorithms perform yield
considerably slow performance in single processors. This slow performance,
coupled with increasingly larger data sets, hinders the execution of existing
algorithms, and obstructs the prototyping of new, more sophisticated
algorithms. Thus, it is becoming imperative to parallelize algorithms and take
advantage of multiple computers with multiple cores. Fortunately, many machine
learning algorithms lend themselves well to parallelization. Unfortunately, the
overhead of creating a distributed system that organizes and manages the work
is a roadblock to parallelizing existing algorithms and prototyping new
ones. We present Qjam, a Python library that abstracts this overhead away,
providing a simple framework that different algorithm implementations can
easily leverage to handle work parallelization.

\section{Distributing Work}

In order for an algorithm to be run in parallel on several machines, some part
of it must be amenable to parallelization. More specifically, some component or
step of the algorithm must be able to be subdivided into \emph{workunits} that
can be processed in parallel by multiple nodes with little or no communication
between the nodes. If communication between nodes is excessive, the overhead of
distributing the work becomes dominant and the any potential performance gained
by running in parallel is lost.

A significant fraction of learning algorithms work in an iterative manner and
depend on generic optimization routines like gradient descent, conjugate
gradient, or L-BFGS. Many of these routines are well suited for parallelization.
As an example, each iteration of batch gradient descent evaluates a function on
every input vector and then sums their results. Consequently, each iteration of
batch gradient descent can easily be parallelized by dividing the sum into $n$
disjoint partial sums, computing each partial sum on a separate slave machine,
and aggregating the results to compute the full sum.

\section{Language \& Framework}

Previous work has been done in distributing MATLAB. However, due to licensing
constraints, this is not feasible for large clusters. We needed to choose
another environment in which to develop and test machine learning
algorithms.

For the implementation of this project, Python and Numpy have been the language
and framework of choice. Our selection criteria included three points: First,
clarity and ease of use; second, networking libraries to enable the
implementation of a distributed framework; and third, performance.

As an alternative, we also considered the use of R for the implementation of
this project. The following is our comparison of Python vs. R and our rationale
for selecting Python.

\subsection{Style Benchmarks}

When prototyping algorithms, the clarity of a language plays a significant role.
MATLAB's simplicity and conciseness, yielding very clear and readable code, can
significantly reduce implementation bugs and ads minimal overhead to expressing
the algorithm in code. This is not the case for languages like C++, where
performance and reliability are more important than readability.

On one hand, R is stylistically very similar to MATLAB. Matrix and list
operations are just as straightforward, and it provides inbuilt equivalents of
most of MATLAB's probability and statistics packages. Further, the R interpreter
makes it just as easy as MATLAB to plot and visualize data structures.

On the other hand, Python's syntax is less suited for matrix operations.
However, Python's Numpy package includes Matlib, an interface that attempts to
emulate MATLAB's syntax. It is designed specifically for MATLAB programmers and
to ease the porting of MATLAB code. Certain syntactic elements are still overly
verbose (e.g. M.transpose() vs M') and may hinder the readability of any given
algorithm.

Strictly from a syntactic and stylistic perspective, R may be preferable to
Python because of its simplicity and closeness to MATLAB. However, because
of the results of the performance and functionality benchmarks presented below,
Python continues to be the best contender.

\subsection{Functionality Benchmarks}

In order to avoid reinventing the wheel, one important consideration in our
selection of a language/framework for this project was the availability of hooks
to existing frameworks for distributed computing (like Hadoop or Disco) and the
existence of networking libraries. Because Python is a far more general
scripting language than R, it is by far the better choice when it comes to
functionality. R is primarily a language for statistical computing, and
therefore lacks a lot of the functionality that may be critical in the
implementation of a distributed system.

\subsection{Performance Benchmarks}

Python is not a mathematical language, but it is easily extensible and provides
numerous packages that provide mathematics specific support; numpy is the most
popular of these packages. Though interpreted, Python can easily call down to C
and offload computationally intensive operations (e.g. numpy's matrix
multiplication is in C), yielding very good performance without sacrificing ease
of use or readability.

We benchmarked the performance of R against Python's (using the Numpy package).
In order to avoid implementation or algorithm-specific bias, we decided to
benchmark common linear algebra functions (e.g. matrix multiplication)
ubiquitous in learning algorithms.

The following tables show the running times of python and R on various
operations using different matrix or vector sizes, as well as the time ratio of
python / R. Every test ran 10000 operations.

\begin{center}
  \vspace{1em}
    \textbf{Matrix Addition} \\
\begin{tabular}{cccc}
size  & python  &  R       & python / R \\
  \hline
50  & 0.0060 & 0.0159 & 0.3774 \\
75  & 0.0110 & 0.0302 & 0.3642 \\
100 & 0.0170 & 0.0519 & 0.3276 \\
150 & 0.0350 & 0.1161 & 0.3015 \\
250 & 0.0950 & 0.3396 & 0.2797 \\
\end{tabular}

\vspace{1em}
    \textbf{Matrix Multiplication} \\
\begin{tabular}{cccc}
size  & python  &  R       & python / R \\
  \hline
50  & 0.1600  & 0.2208  & 0.7246 \\
75  & 0.5800  & 0.7339  & 0.7903 \\
100 & 1.3030  & 1.6323  & 0.7983 \\
150 & 4.2350  & 5.2311  & 0.8096 \\
250 & 18.9190 & 22.9759 & 0.8234 \\
\end{tabular}

\vspace{1em}
    \textbf{Element-Wise Matrix Multiplication} \\
\begin{tabular}{cccc}
size  & python  &  R       & python / R \\
  \hline
150   & 0.0350  &  0.1576  &  0.2221 \\
225   & 0.0760  &  0.3741  &  0.2032 \\
300   & 0.1510  &  0.6859  &  0.2201 \\
450   & 0.9310  &  2.0938  &  0.4446 \\
750   & 3.3010  &  5.4117  &  0.6100 \\
\end{tabular}

\vspace{1em}
    \textbf{Matrix Transpose} \\
\begin{tabular}{cccc}
size  & python  &  R       & python / R \\
  \hline
50  & 0.0010  & 0.0325 & 0.0308 \\
75  & 0.0010  & 0.0610 & 0.0164 \\
100 & 0.0010  & 0.1030 & 0.0097 \\
150 & 0.0010  & 0.2196 & 0.0046 \\
250 & 0.0010  & 0.6119 & 0.0016 \\
\end{tabular}

\vspace{1em}
\textbf{Vector inner product} \\
\begin{tabular}{cccc}
size  & python  &  R       & python / R \\
  \hline
2500  & 0.0040 & 0.0523 & 0.0765 \\
3750  & 0.0060 & 0.0772 & 0.0777 \\
5000  & 0.0070 & 0.1030 & 0.0680 \\
7500  & 0.0100 & 0.1519 & 0.0658 \\
12500 & 0.0160 & 0.2514 & 0.0636 \\
\end{tabular}

\end{center}

In terms of performance, Python is the clear winner. It outperforms R in every
case, most of the cases by an order of magnitude. In the worst case, Python
takes 80\% of the time that R takes.

Perhaps also some words here about why Python is still acceptable even if it is
significantly slower than MATLAB. In particular, we are doing rapid
prototyping, so peak CPU performance isn't the deciding factor. We are more
interested in processing lots of data by distributing it to enough nodes such
that the data can fit in memory and be processed effectively.


\section{Framework Design}

% \subsection{Conventional Approaches}
%
% Some words here about how the batch gradient descent example easily lends
% itself to a straight mapreduce workflow. Because of this workflow, we first
% considered using an existing mapreduce framework for our project.
%

% One such framework we considered is Hadoop MapReduce. Some words here about
% why Hadoop doesn't apply well to our problem. Consider the batch gradient
% decent example again. Each iteration is essentially a mapreduce operation:
% the examples are split up and distributed, the individual sums are calculated
% at each node, and the reduce operation is calculating the final sum. The
% examples considered at each node must remain stored locally at that node,
% ideally in memory, for later recall during the next mapreduce operation for
% the subsequent iteration.  Some words here about Disco and its attempt to
% solve this problem?

\subsection{Our design}

Our design consists of a single master and one or more slaves, as shown in
Fig. \ref{diagram}.

\begin{figure}[hb]
  \begin{center}
    \includegraphics[width=2.5in]{fwk_diagram/fwk_diagram.pdf}
  \end{center}
  \caption{Diagram of our framework.}
  \label{diagram}
\end{figure}

First, on the master, you define a Python map function with the following
prototype:
\begin{verbatim}
 def mymap(examples, params):
   psum = 0
   for (xi,yi) in examples:
       # ...
       psum += (yi - g(params.theta.T * xi)) * xi
   return psum
\end{verbatim}

Then, also on the master, you specify your training examples and initial
parameters:
\begin{verbatim}
 examples = ((x1, y1), (x2, y2), (x3, y3))
 params   = (0, 0, 0)
\end{verbatim}

The difference between examples and parameters is that examples will not be
resent on each iteration. We will check whether the slave already has the {\tt
  examples} object (with the same hash) and only resend it if needed. The
parameters data, however, will be resent for each iteration. These two objects
can be arbitrarily typed, as long as they are serializable. For example, for
large datasets, we might implement a list/dictionary proxy (that behaves like
its standard Python counterpart but doesn't store everything in memory). Also,
typically they will hold {\tt numpy} arrays and matrices instead of Python
tuples as shown in the example above.

To start the job, call:

\begin{verbatim}
 master.run(mymap, examples, params)
\end{verbatim}

This instructs the master to serialize the map function, training data, and
initial parameters, and send them to the slaves, each of which is listening for
jobs. Each slave will run the {\tt mymap} function, with the parameters in {\tt
  params} and some portion of the training set available in {\tt examples}.

This call will block until all slaves respond with their partial sums. The {\tt
  run} method will apply the {\tt sum} reduce function to each slave's response
and will return the total sum. (For easy local testing, the {\tt mymap}
function can also be run on the local machine in the same Python process.)

We will not address slave/network failure tolerance for now. This can be added
later, and we don't anticipate it being a major problem for the small clusters
we are initially targeting.

\subsection{Serialization and Slave-Local Storage}

We initially aim to support training sets around 30 GB, so we must have a way
to distribute this dataset to the slaves without having to send it to each
slave on each iteration. In the MapReduce model we're adopting, {\tt map}
function only relies on data given to it; there is no global data. This lets
the master control distribution of the data using its superior knowledge about
the cluster size and capabilities, and it reduces data transmission overhead.

When the master runs a job on $n$ slaves, it creates $n$ equal slices of the
training set and assigns one to each slave. On subsequent iterations, each
slave will be assigned the same data slice, as long as no slaves join or leave
the cluster. The master then sends the data slice to the slave, but only if the
slave doesn't already have the data.

We expect that the initial serialization and transmission of the training set
to each slave will take quite a while. This is an area ripe for future
optimization. However, we won't naively succumb to the vice of premature
optimization unless this is a major bottleneck.

Each slave will know what data slice it has, and will use that data slice as an
argument to the {\tt map} function.

This is similar to the SlaveRefs in Adam Coates' Matlab parallel framework, but
we are able to use Python's superior syntax to hide the implementation details
of references to data on slaves. Also, our system as described will only send
portions of the data to each slave, which will decrease data transmission
overhead.

\section{Benchmarks}

We benchmarked the framework running various algorithms with multiple workers.

\subsection{L-BFGS Sparse Auto-Encoder}

We benchmarked qjam using a sparse autoencoder with L-BFGS. A sparse
autoencoder is an unsupervised learning algorithm that automatically learns
features from unlabeled data. It is implemented as a neural network with one
hidden layer (parameters) that adjusts its weight values at each iteration over
the training set. L-BFGS is a limited-memory, quasi-Newton optimization method
for unconstrained optimization.

We benchmarked the running time of the sparse autoencoder using a parallelized
cost function (with L-BFGS optimizing it). We tested a regular single-core
implementation against 2, 4, 8, and 16 workers over four multicore machines. We
tested with three datasets (of sizes 1000, 10 000, and 100 000). We obtained
the following results:

\begin{center}

\textbf{Iteration Mean Time (seconds)} \\
\begin{tabular}{cccc}
workers  & 1k  &  10k   & 100k   \\
  \hline
1  & 0.1458 & 0.7310 & 10.0282 \\
2  & 0.1752 & 0.3321 & 4.6782 \\
4  & 0.2634 & 0.3360 & 2.4858 \\
8  & 0.5339 & 0.5251 & 1.8046 \\
16 & 0.9969 & 1.0186 & 1.5862 \\
\end{tabular}

\vspace{1em}
\textbf{Total Running Time (seconds)} \\
\begin{tabular}{cccc}
workers  & 1k  &  10k   & 100k   \\
  \hline
1  & 76 (1.00) & 370 (1.00) & 5030 (1.00) \\
2  & 92 (1.21) & 170 (0.46) & 2350 (0.47) \\
4  & 137 (1.80) & 173 (0.46) & 1253 (0.25) \\
8  & 275 (3.61) & 270 (0.73) & 914 (0.18) \\
16 & 544 (7.51) & 529 (1.43) & 703 (0.14) \\
\end{tabular}
\end{center}

The running times show a remarkable speedup when using qjam with multiple
workers. For our longest run, we saw a drop to under 14\% of the single-core's
running time. For big jobs (10,000, 100,000), qjam performs very well, beating
the single-core every time. For non-intensive jobs (1000), the overhead of many
workers can drive the performance beyond that of the single-core's
implementation.

This leads to another observation: a particular number of workers seems to be
suited for a particular job size: for the 10,000 patches runs, the best run was
that with 2 workers. Though the others still performed better then the single
core, they performed worse. For the largest job, though the 16 worker runtime
was the lowest, the savings from 8 workers to 16 were small. This further
confirms that the number of workers should be picked according to the job size,
to minimize the overhead of distributing the job.

% do the biliography:
%\bibliographystyle{IEEEbib}
%\bibliography{my-bibliography-file}

%----------------------------------------------------------------------
% FIGURES
%----------------------------------------------------------------------
% There are many ways to include figures in the text. We will assume
% that the figure is some sort of EPS file.
%
% The outdated packages epsfig and psfig allow you to insert figures
% like: \psfig{filename.eps} These should really be done now using the
% \includegraphics{filename.eps} command.
%
% i.e.,
%
% \includegraphics{file.eps}
%
% whenever you want to include the EPS file 'file.eps'. There are many
% options for the includegraphics command, and are outlined in the
% on-line documentation for the "graphics bundle". Using the options,
% you can specify the height, total height (height+depth), width, scale,
% angle, origin, bounding box "bb",view port, and can trim from around
% the sides of the figure. You can also force LaTeX to clip the EPS file
% to the bounding box in the file. I find that I often use the scale,
% trim and clip commands.
%
% \includegraphics[scale=0.6,trim=0 0 0 0,clip=]{file.eps}
%
% which magnifies the graphics by 0.6 (If I create a graphics for an
% overhead projector transparency, I find that a magnification of 0.6
% makes it look much better in a paper), trims 0 points off
% of the left, bottom, right and top, and clips the graphics. If the
% trim numbers are negative, space is added around the figure. This can
% be useful to help center the graphics, if the EPS file bounding box is
% not quite right.
%
% To center the graphics,
%
% \begin{center}
% \includegraphics...
% \end{center}
%
% I have not yet written good documentation for this, but another
% package which helps in figure management is the package ieeefig.sty,
% available at: http://www-isl.stanford.edu/people/glp/ieee.shtml
% Specify:
%
%\usepackage{ieeefig}
%
% in the preamble, and whenever you want a figure,
%
%\figdef{filename}
%
% where, filename.tex is a LaTeX file which defines what the figure is.
% It may be as simple as
%
% \inserteps{filename.eps}
%
% or
% \inserteps[includegraphics options]{filename.eps}
%
% or may be a very complicated LaTeX file.

\end{document}
