\documentclass[%
        %draft,
        %submission,
        %compressed,
        final,
        %
        %technote,
        %internal,
        %submitted,
        %inpress,
        %reprint,
        %
        %titlepage,
        notitlepage,
        %anonymous,
        narroweqnarray,
        inline,
        %twoside,
        ]{ieee}
%
% some standard modes are:
%
% \documentclass[draft,narroweqnarray,inline]{ieee}
% \documentclass[submission,anonymous,narroweqnarray,inline]{ieee}
% \documentclass[final,narroweqnarray,inline]{ieee}

\usepackage{ieeefig,url,enumerate}

\begin{document}

\title{Scaling Machine Learning Algorithms}

% format author this way for journal articles.
\author[SHORT NAMES]{
  Juan Batiz-Benet \\
  \and{\quad\quad}
  Quinn Slack \\
  \and{\quad\quad}
  Matt Sparks \\
  \and{\quad\quad}
  Ali Yahya
}

% specifiy the journal name
%\journal{IEEE Transactions on Something, 1997}

% Or, when the paper is a preprint, try this...
%\journal{IEEE Transactions on Something, 1997, TN\#9999.}

% Or, specify the conference place and date.
%\confplacedate{Ottawa, Canada, May 19--21, 1997}

\maketitle

\begin{abstract}
We wish to provide a portable framework for the rapid prototyping of machine
learning algorithms on a cluster of computers.
\end{abstract}

% do the keywords
%\begin{keywords}
%keyword 1, keyword 2 ...
%\end{keywords}

\section{Introduction}

% try out a theorem...
\newtheorem{theorem}{Theorem}

\begin{theorem}[Theorem name]
  Consider the system ...
\end{theorem}

\begin{proof}
  The proof is trivial.
\end{proof}

\section{Distributing Work}

In order for an algorithm to be run in parallel on several machines, some part
of it must be amenable to parallelization. More specifically, some component or
step of the algorithm must be able to be subdivided into \emph{workunits} that
can be processed in parallel by multiple nodes with little or no communication
between the nodes. If communication between nodes is excessive, the overhead of
distributing the work becomes dominant and the any potential performance gained
by running in parallel is lost.

A significant fraction of learning algorithms work in an iterative manner and
depend on generic optimization routines like gradient descent, conjugate
gradient, or L-BFGS. Many of these routines are well suited for parallelization.
As an example, let's consider batch gradient descent.

\section{Rapid Prototyping}

Previous work has been done in distributing Matlab. However, due to licensing
constraints, this is not feasible for large clusters. We needed to choose
another environment in which to develop and test machine learning
algorithms. For this project, we considered Python and R as alternatives to
Matlab.

\subsection{Requirements}

Words here about why R and Python might be good choices. (Packages available
for matrix manipulations, highly general, free, easy to pick up and use,
portable?)


\subsection{Style Benchmarks}

When prototyping algorithms, the clarity of a language plays a significant role.  Matlab's simplicity and conciseness, yielding very clear and readable code, can significantly reduce implementation bugs and ads minimal overhead to expressing the algorithm in code. This is not the case for languages like C++, where performance and reliability are more important than readability.

In terms of style, Python synax is often praised for its simplicity. For our purposes, Python's Numpy package includes Matlib, an interface that attempts to emulate Matlab's syntax. It is designed  specifically for Matlab programmers and to ease the porting of Matlab code. Some syntax is still overly verbose (e.g. M.transpose() vs M') and hinders the clarity of the language, but overall seems clear enough, and makes porting existing Matlab implementations easy.

Some words about R syntax (looks weird at first glance, but others know best.)

Conclusion on style. Is python better than R? seems to me.

\subsection{Performance Benchmarks}

We benchmarked the performance of R against Python's (using the numpy package). In order to avoid implementation or algorithm-specific bias, we decided to benchmark common linear algebra functions (e.g. matrix multiplication) ubiquitous in learning algorithms.

The following tables show the running times of python and R on various operations using different matrix or vector sizes, as well as the time ratio of python / R. Every test ran 10000 operations.

\begin{center}
  \vspace{1em}
    \textbf{Matrix Addition} \\
\begin{tabular}{cccc}
size  & python  &  R       & python / R \\
  \hline
50  & 0.0060 & 0.0159 & 0.3774 \\
75  & 0.0110 & 0.0302 & 0.3642 \\
100 & 0.0170 & 0.0519 & 0.3276 \\
150 & 0.0350 & 0.1161 & 0.3015 \\
250 & 0.0950 & 0.3396 & 0.2797 \\
\hline
\end{tabular}

\vspace{1em}
    \textbf{Matrix Multiplication} \\
\begin{tabular}{cccc}
size  & python  &  R       & python / R \\
  \hline
50  & 0.1600  & 0.2208  & 0.7246 \\
75  & 0.5800  & 0.7339  & 0.7903 \\
100 & 1.3030  & 1.6323  & 0.7983 \\
150 & 4.2350  & 5.2311  & 0.8096 \\
250 & 18.9190 & 22.9759 & 0.8234 \\
\end{tabular}

\vspace{1em}
    \textbf{Element-Wise Matrix Multiplication} \\
\begin{tabular}{cccc}
size  & python  &  R       & python / R \\
  \hline
150   & 0.0350  &  0.1576  &  0.2221 \\
225   & 0.0760  &  0.3741  &  0.2032 \\
300   & 0.1510  &  0.6859  &  0.2201 \\
450   & 0.9310  &  2.0938  &  0.4446 \\
750   & 3.3010  &  5.4117  &  0.6100 \\
\end{tabular}

\vspace{1em}
    \textbf{Matrix Transpose} \\
\begin{tabular}{cccc}
size  & python  &  R       & python / R \\
  \hline
50  & 0.0010  & 0.0325 & 0.0308 \\
75  & 0.0010  & 0.0610 & 0.0164 \\
100 & 0.0010  & 0.1030 & 0.0097 \\
150 & 0.0010  & 0.2196 & 0.0046 \\
250 & 0.0010  & 0.6119 & 0.0016 \\
\end{tabular}

\vspace{1em}
\textbf{Vector inner product} \\
\begin{tabular}{cccc}
size  & python  &  R       & python / R \\
  \hline
2500  & 0.0040 & 0.0523 & 0.0765 \\
3750  & 0.0060 & 0.0772 & 0.0777 \\
5000  & 0.0070 & 0.1030 & 0.0680 \\
7500  & 0.0100 & 0.1519 & 0.0658 \\
12500 & 0.0160 & 0.2514 & 0.0636 \\
\end{tabular}

\end{center}

In terms of performance, Python is the clear winner. It outperforms R in every case, most of the cases by an order of magnitude. In the worst case, Python takes 80\% of the time that R takes.

Perhaps also some words here about why Python is still acceptable even if it is
significantly slower than Matlab. In particular, we are doing rapid
prototyping, so peak CPU performance isn't the deciding factor. We are more
interested in processing lots of data by distributing it to enough nodes such
that the data can fit in memory and be processed effectively.

\section{Framework Design}

\subsection{Conventional Approaches}

Some words here about how the batch gradient descent example easily lends
itself to a straight mapreduce workflow. Because of this workflow, we first
considered using an existing mapreduce framework for our project.

One such framework we considered is Hadoop MapReduce. Some words here about why
Hadoop doesn't apply well to our problem. Consider the batch gradient decent
example again. Each iteration is essentially a mapreduce operation: the
examples are split up and distributed, the individual sums are calculated at
each node, and the reduce operation is calculating the final sum. The examples
considered at each node must remain stored locally at that node, ideally in
memory, for later recall during the next mapreduce operation for the subsequent
iteration.

Some words here about Disco and its attempt to solve this problem?

\subsection{Our design}

Our design consists of a single master and one or more slaves, as shown in
Fig. \ref{diagram}.

\begin{figure}[hb]
  \begin{center}
    \includegraphics[width=2.5in]{fwk_diagram/fwk_diagram.pdf}
  \end{center}
  \caption{Diagram of our framework.}
  \label{diagram}
\end{figure}

Description of the library here. The rapidly prototyped algorithms call into it
and tell it to do work.

\subsection{Library Semantics}

Calls to the library are blocking. A call like,

\begin{verbatim}
    master.execute(func, theta, local_data)
\end{verbatim}

will not return until all work has been performed.

Our failure semantics are simple. Initially, we are not focusing on slave
failure. However, it is easy to add in later (reship the local data, rerun dead
slave's work). The master shares fate with the main program being run; if the
main program dies, the master dies with it, and vice versa. Thus, we do not
need replicated masters or anything similarly sophisticated.

\subsection{Serialization and Local Storage}

Some notes here about pickling functions and managing local storage at each
node.

The master divides the data and ships to each slave its share of the examples
(or whatever other data is needed).

% do the biliography:
%\bibliographystyle{IEEEbib}
%\bibliography{my-bibliography-file}

%----------------------------------------------------------------------
% FIGURES
%----------------------------------------------------------------------
% There are many ways to include figures in the text. We will assume
% that the figure is some sort of EPS file.
%
% The outdated packages epsfig and psfig allow you to insert figures
% like: \psfig{filename.eps} These should really be done now using the
% \includegraphics{filename.eps} command.
%
% i.e.,
%
% \includegraphics{file.eps}
%
% whenever you want to include the EPS file 'file.eps'. There are many
% options for the includegraphics command, and are outlined in the
% on-line documentation for the "graphics bundle". Using the options,
% you can specify the height, total height (height+depth), width, scale,
% angle, origin, bounding box "bb",view port, and can trim from around
% the sides of the figure. You can also force LaTeX to clip the EPS file
% to the bounding box in the file. I find that I often use the scale,
% trim and clip commands.
%
% \includegraphics[scale=0.6,trim=0 0 0 0,clip=]{file.eps}
%
% which magnifies the graphics by 0.6 (If I create a graphics for an
% overhead projector transparency, I find that a magnification of 0.6
% makes it look much better in a paper), trims 0 points off
% of the left, bottom, right and top, and clips the graphics. If the
% trim numbers are negative, space is added around the figure. This can
% be useful to help center the graphics, if the EPS file bounding box is
% not quite right.
%
% To center the graphics,
%
% \begin{center}
% \includegraphics...
% \end{center}
%
% I have not yet written good documentation for this, but another
% package which helps in figure management is the package ieeefig.sty,
% available at: http://www-isl.stanford.edu/people/glp/ieee.shtml
% Specify:
%
%\usepackage{ieeefig}
%
% in the preamble, and whenever you want a figure,
%
%\figdef{filename}
%
% where, filename.tex is a LaTeX file which defines what the figure is.
% It may be as simple as
%
% \inserteps{filename.eps}
%
% or
% \inserteps[includegraphics options]{filename.eps}
%
% or may be a very complicated LaTeX file.

\end{document}
