\documentclass[%
  %draft,
  %submission,
  %compressed,
  final,
  %
  %technote,
  %internal,
  %submitted,
  %inpress,
  %reprint,
  %
  %titlepage,
  notitlepage,
  %anonymous,
  narroweqnarray,
  inline,
  %twoside,
]{ieee}

\usepackage{ieeefig, url, enumerate}

\begin{document}

\title{Parallelizing Machine Learning Algorithms}

\author[SHORT NAMES]{
  \begin{tabular*}{0.75\textwidth}{@{\extracolsep{\fill}}cccc}
    Juan Batiz-Benet & Quinn Slack & Matt Sparks & Ali Yahya \\
    \multicolumn{4}{c}{
      \normalsize
      \url{{jbenet,sqs,msparks,alive}@cs.stanford.edu}}
  \end{tabular*}
}

\maketitle

\begin{abstract}
Qjam provides a portable framework for the rapid prototyping of machine
learning algorithms on a cluster of computers.
\end{abstract}

\section{Introduction}

The sheer number of computations that Machine Learning algorithms perform yield
considerably slow performance in single processors. This slow performance,
coupled with increasingly larger data sets, hinders the execution of existing
algorithms, and obstructs the prototyping of new, more sophisticated
algorithms. Thus, it is becoming imperative to parallelize algorithms and take
advantage of multiple computers with multiple cores. Fortunately, many machine
learning algorithms lend themselves to parallelization. However, the
overhead of creating a distributed system that organizes and manages the work
is a roadblock to parallelizing existing algorithms and prototyping new
ones. We present Qjam, a Python library that abstracts this overhead away,
providing a simple framework that different algorithm implementations can
easily leverage to parallelize work across a number of machines and processors.

\section{Previous Work}

Significant research has been done in the area of distributed data
processing. Perhaps the most notable and relevant contribution is the MapReduce
programming model~\cite{mapreduce}, which applies the \textsl{map} and
\textsl{reduce} functions from functional programming to large datasets spread
over a cluster of machines. Since their introduction, the MapReduce concepts
have been implemented in several projects for highly parallel computing, such
as Apache Hadoop~\cite{hadoop}.

Chu et al.~\cite{chu2007map} show how ten popular machine learning algorithms
can be written in a ``summation form'' in which parallelization is
straightforward. The authors implemented these algorithms on a MapReduce-like
framework and ran them on multicore machines. They yielded a near-linear
speedup as the number of cores was increased.

Whereas Chu et al. experimented on single multicore machines, our project
extends their ideas to a cluster of networked computers. Rather than utilize a
framework like Hadoop, which is more oriented for large batch processing jobs,
we have designed and implemented a lightweight framework for low-latency tasks.

\section{Choosing a Language}

One of the goals of this project is to enable rapid prototyping of machine
learning algorithms. Thus, we must support a language that allows fast
development. Further, support for linear algebra operations must be available
natively or through a library.

C++ is known to provide excellent performance, but it does not satisfy our
rapid prototyping requirement. MATLAB is an excellent candidate for
implementing machine learning algorithms, but unfortunately, licensing
constraints make it infeasible for use on large clusters. For this project, we
evaluated Python as R as possible options for our library, and chose
Python.

The following sections compare Python with R and detail our rationale for
selecting Python for this project.

\subsection{Code Style}

When prototyping algorithms, the clarity of a language plays a significant
role. MATLAB's simplicity and conciseness, yielding very clear and readable
code, can significantly reduce implementation bugs and adds minimal overhead to
expressing the algorithm in code. This is not the case for languages like C++,
where performance and reliability are more important than readability.

R is stylistically very similar to MATLAB. Matrix and list operations are just
as straightforward, and it provides inbuilt equivalents of most of MATLAB's
probability and statistics packages. Further, the R interpreter makes it just
as easy as MATLAB to plot and visualize data structures.

While Python's syntax is less suited for matrix operations, the NumPy
package~\cite{numpy} for Python includes Matlib, an interface that attempts to
emulate MATLAB's syntax. It is designed specifically for MATLAB programmers and
to ease the porting of MATLAB code. Certain syntactic elements are still overly
verbose, e.g. \texttt{M.transpose()} vs \texttt{M'}, and may hinder the
readability of an algorithm.

Strictly from a syntactic and stylistic perspective, R may be preferable to
Python because of its simplicity and closeness to MATLAB. However, because of
the results of the performance and functionality benchmarks presented below,
Python continues to be the best contender.

\subsection{Functionality}

We had two options when implementing our framework. We could write the
internals in something like C++ and provide a loadable extension to Python or
R, or we could implement the internals in the scripting language itself. Each
approach has its advantages, but under time pressure, the latter approach is
safer. Because Python is a far more general scripting language than R, it is by
far the better choice with respect to functionality. R is primarily a language
for statistical computing, and therefore lacks a lot of the functionality that
may be critical in the implementation of a distributed system.

\subsection{Performance}

Python is not a mathematical language, but it is easily extensible and provides
numerous packages that provide mathematics-specific support; NumPy is the most
popular of these packages. Though interpreted, Python can easily call down to C
and offload computationally-intensive operations. For instance, NumPy's matrix
multiplication is interally implemented in C, yielding very good performance
without sacrificing ease of use or readability in the Python code.

We benchmarked the performance of R against Python's (using the NumPy package).
In order to avoid implementation or algorithm-specific bias, we decided to
benchmark common linear algebra functions, such as matrix multiplication,
ubiquitous in learning algorithms.

Table \ref{PythonvsR} shows the running times of Python and R on various
operations using different matrix or vector sizes, as well as the time ratio of
$Python / R$. Every test ran 10,000 operations.

\begin{table}[h!]
  \begin{center}
    \vspace{1em}
    \textbf{Matrix Addition} \\
    \begin{tabular}{cccc}
      Size  & Python  &  R      & Python / R \\
      \hline
      50  & 0.0060 & 0.0159 & 0.3774 \\
      75  & 0.0110 & 0.0302 & 0.3642 \\
      100 & 0.0170 & 0.0519 & 0.3276 \\
      150 & 0.0350 & 0.1161 & 0.3015 \\
      250 & 0.0950 & 0.3396 & 0.2797 \\
    \end{tabular}

    \vspace{1em}
    \textbf{Matrix Multiplication} \\
    \begin{tabular}{cccc}
      Size  & Python  &  R    & Python / R \\
      \hline
      50  & 0.1600  & 0.2208  & 0.7246 \\
      75  & 0.5800  & 0.7339  & 0.7903 \\
      100 & 1.3030  & 1.6323  & 0.7983 \\
      150 & 4.2350  & 5.2311  & 0.8096 \\
      250 & 18.9190 & 22.9759 & 0.8234 \\
    \end{tabular}

    \vspace{1em}
    \textbf{Element-Wise Matrix Multiplication} \\
    \begin{tabular}{cccc}
      Size  & Python  &  R       & Python / R \\
      \hline
      150   & 0.0350  &  0.1576  &  0.2221 \\
      225   & 0.0760  &  0.3741  &  0.2032 \\
      300   & 0.1510  &  0.6859  &  0.2201 \\
      450   & 0.9310  &  2.0938  &  0.4446 \\
      750   & 3.3010  &  5.4117  &  0.6100 \\
    \end{tabular}

    \vspace{1em}
    \textbf{Matrix Transpose} \\
    \begin{tabular}{cccc}
      Size  & Python  &  R       & Python / R \\
      \hline
      50  & 0.0010  & 0.0325 & 0.0308 \\
      75  & 0.0010  & 0.0610 & 0.0164 \\
      100 & 0.0010  & 0.1030 & 0.0097 \\
      150 & 0.0010  & 0.2196 & 0.0046 \\
      250 & 0.0010  & 0.6119 & 0.0016 \\
    \end{tabular}

    \vspace{1em}
    \textbf{Vector inner product} \\
    \begin{tabular}{cccc}
      Size  & Python &  R       & Python / R \\
      \hline
      2500  & 0.0040 & 0.0523 & 0.0765 \\
      3750  & 0.0060 & 0.0772 & 0.0777 \\
      5000  & 0.0070 & 0.1030 & 0.0680 \\
      7500  & 0.0100 & 0.1519 & 0.0658 \\
      12500 & 0.0160 & 0.2514 & 0.0636 \\
    \end{tabular}
  \end{center}
  \caption{Benchmarks of Python and R for linear algebra
    operations.}
  \label{PythonvsR}
\end{table}

In terms of performance, Python is the clear winner. It outperforms R in every
case, most of the cases by an order of magnitude. In the worst case, Python
takes 82\% of the time that R takes.

%Perhaps also some words here about why Python is still acceptable even if it
%is significantly slower than MATLAB. In particular, we are doing rapid
%prototyping, so peak CPU performance isn't the deciding factor. We are more
%interested in processing lots of data by distributing it to enough nodes such
%that the data can fit in memory and be processed effectively.

\section{Architecture}

This section describes the architecture of the qjam framework. Subsection
\ref{Components} defines the major components of the system and how they
communicate. Subsection \ref{Library} explains the programming
interface. Finally, Subsection \ref{Library} explains our \texttt{DataSet}
objects and how they are used to divide data among workers.

\subsection{Components}
\label{Components}

Qjam is a single-master distributed system made up of instances of the
following components: \\

\begin{description}
  \item[Worker] --- The Worker is a program that is copied to all of the remote
    machines during the bootstrapping process. It is responsible for waiting
    for instructions from the Master, and upon receiving work, processing that
    work and returning the result. \\

  \item[RemoteWorker] --- The RemoteWorker is a special Python class that
    communicates with the remote machines. One RemoteWorker has a single target
    machine that can be reached via ssh. There can be many RemoteWorkers with
    the same target (say, in the case where there are many cores on a machine),
    but only one target per RemoteWorker. At creation, the RemoteWorker
    bootstraps the remote machine by copying the requisite files to run the
    Worker program, via ssh. After the bootstrapping process completes, the
    RemoteWorker starts a Worker process on the remote machine and attaches to
    it. The RemoteWorker is the proxy between the Master and the Worker. \\

  \item[Master] --- The Master is a Python class that divides up work and
    assigns the work units among its pool of RemoteWorker instances. These
    RemoteWorker instances relay the work to the Worker programs running on the
    remote machines and wait for the results. \\
\end{description}

Fig. \ref{diagram} shows the communication channels between components on
multiple machines.

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=3.3in]{fwk_diagram/fwk_diagram.pdf}
  \end{center}
  \caption{Master controlling four RemoteWorkers with Workers in two machines.}
  \label{diagram}
\end{figure}

\subsection{Qjam Library API}

TODO. Use Listings package.

\subsection{DataSet Objects}

The difference between examples and parameters is that examples will not be
resent on each iteration. We will check whether the slave already has the {\tt
  examples} object (with the same hash) and only resend it if needed. The
parameters data, however, will be resent for each iteration. These two objects
can be arbitrarily typed, as long as they are serializable. For example, for
large datasets, we might implement a list/dictionary proxy (that behaves like
its standard Python counterpart but doesn't store everything in memory). Also,
typically they will hold {\tt numpy} arrays and matrices instead of Python
tuples as shown in the example above.

To start the job, call:

\begin{verbatim}
 master.run(mymap, examples, params)
\end{verbatim}

This instructs the master to serialize the map function, training data, and
initial parameters, and send them to the slaves, each of which is listening for
jobs. Each slave will run the {\tt mymap} function, with the parameters in {\tt
  params} and some portion of the training set available in {\tt examples}.

This call will block until all slaves respond with their partial sums. The {\tt
  run} method will apply the {\tt sum} reduce function to each slave's response
and will return the total sum. (For easy local testing, the {\tt mymap}
function can also be run on the local machine in the same Python process.)

We will not address slave/network failure tolerance for now. This can be added
later, and we don't anticipate it being a major problem for the small clusters
we are initially targeting.

\subsection{Serialization and Slave-Local Storage}

We initially aim to support training sets around 30 GB, so we must have a way
to distribute this dataset to the slaves without having to send it to each
slave on each iteration. In the MapReduce model we're adopting, {\tt map}
function only relies on data given to it; there is no global data. This lets
the master control distribution of the data using its superior knowledge about
the cluster size and capabilities, and it reduces data transmission overhead.

When the master runs a job on $n$ slaves, it creates $n$ equal slices of the
training set and assigns one to each slave. On subsequent iterations, each
slave will be assigned the same data slice, as long as no slaves join or leave
the cluster. The master then sends the data slice to the slave, but only if the
slave doesn't already have the data.

We expect that the initial serialization and transmission of the training set
to each slave will take quite a while. This is an area ripe for future
optimization. However, we won't naively succumb to the vice of premature
optimization unless this is a major bottleneck.

Each slave will know what data slice it has, and will use that data slice as an
argument to the {\tt map} function.

This is similar to the SlaveRefs in Adam Coates' Matlab parallel framework, but
we are able to use Python's superior syntax to hide the implementation details
of references to data on slaves. Also, our system as described will only send
portions of the data to each slave, which will decrease data transmission
overhead.

\section{Evaluation}

We benchmarked the framework running various algorithms with multiple workers.

\subsection{L-BFGS Sparse Auto-Encoder}

We benchmarked qjam using a sparse autoencoder with L-BFGS~\cite{lbfgs}. A
sparse autoencoder is an unsupervised learning algorithm that automatically
learns features from unlabeled data. It is implemented as a neural network with
one hidden layer (parameters) that adjusts its weight values at each iteration
over the training set. L-BFGS is a limited-memory, quasi-Newton optimization
method for unconstrained optimization.

We benchmarked the running time of the sparse autoencoder using a parallelized
cost function (with L-BFGS optimizing it). We tested a regular single-core
implementation against 2, 4, 8, and 16 workers over four multicore machines. We
tested with three datasets (of sizes 1000, 10 000, and 100 000). We obtained
the following results:

% TODO(ms): use table environment here with a label.
\begin{center}

\textbf{Iteration Mean Time (seconds)} \\
\begin{tabular}{cccc}
workers  & 1k  &  10k   & 100k   \\
  \hline
1  & 0.1458 & 0.7310 & 10.0282 \\
2  & 0.1752 & 0.3321 & 4.6782 \\
4  & 0.2634 & 0.3360 & 2.4858 \\
8  & 0.5339 & 0.5251 & 1.8046 \\
16 & 0.9969 & 1.0186 & 1.5862 \\
\end{tabular}

\vspace{1em}
\textbf{Total Running Time (seconds)} \\
\begin{tabular}{cccc}
workers  & 1k  &  10k   & 100k   \\
  \hline
1  & 76 (1.00) & 370 (1.00) & 5030 (1.00) \\
2  & 92 (1.21) & 170 (0.46) & 2350 (0.47) \\
4  & 137 (1.80) & 173 (0.46) & 1253 (0.25) \\
8  & 275 (3.61) & 270 (0.73) & 914 (0.18) \\
16 & 544 (7.51) & 529 (1.43) & 703 (0.14) \\
\end{tabular}
\end{center}

The running times show a remarkable speedup when using qjam with multiple
workers. For our longest run, we saw a drop to under 14\% of the single-core's
running time. For big jobs (10,000, 100,000), qjam performs very well, beating
the single-core every time. For non-intensive jobs (1000), the overhead of many
workers can drive the performance beyond that of the single-core's
implementation.

This leads to another observation: a particular number of workers seems to be
suited for a particular job size: for the 10,000 patches runs, the best run was
that with 2 workers. Though the others still performed better then the single
core, they performed worse. For the largest job, though the 16 worker runtime
was the lowest, the savings from 8 workers to 16 were small. This further
confirms that the number of workers should be picked according to the job size,
to minimize the overhead of distributing the job.

\section{Future Work}

\section{Conclusion}

\bibliographystyle{IEEEbib}
\bibliography{report}

\end{document}
