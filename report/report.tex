\documentclass[%
  %draft,
  %submission,
  %compressed,
  final,
  %
  %technote,
  %internal,
  %submitted,
  %inpress,
  %reprint,
  %
  %titlepage,
  notitlepage,
  %anonymous,
  narroweqnarray,
  inline,
  %twoside,
]{ieee}

\usepackage{ieeefig, url, enumerate, subfigure}

\begin{document}

\title{Parallelizing Machine Learning Algorithms}

\author[SHORT NAMES]{
  \begin{tabular*}{0.75\textwidth}{@{\extracolsep{\fill}}cccc}
    Juan Batiz-Benet & Quinn Slack & Matt Sparks & Ali Yahya \\
    \multicolumn{4}{c}{
      \normalsize
      \url{{jbenet,sqs,msparks,alive}@cs.stanford.edu}}
  \end{tabular*}
}

\maketitle

\begin{abstract}
Implementing machine learning algorithms involves of performing computationally
intensive operations on large data sets. As these data sets grow in size and
algorithms grow in complexity, it becomes necessary to spread the work among
multiple computers and multiple cores. Qjam is a framework for the rapid
prototyping of parallel machine learning algorithms on clusters.
\end{abstract}

\section{Introduction}
Many machine learning algorithms are easy to parallelize in theory. However,
the fixed cost of creating a distributed system that organizes and manages the
work is an obstacle to parallelizing existing algorithms and prototyping new
ones. We present Qjam, a Python library that transparently parallelizes
machine learning algorithms that adhere to a constrained MapReduce model of
computation.
\section{Previous Work}

Significant research has been done in the area of distributed data
processing. Perhaps the most notable and relevant contribution is the MapReduce
programming model~\cite{mapreduce}, which applies the \textsl{map} and
\textsl{reduce} functions from functional programming to large datasets spread
over a cluster of machines. Since their introduction, the MapReduce concepts
have been implemented in several projects for highly parallel computing, such
as Apache Hadoop~\cite{hadoop}.

Chu et al.~\cite{chu2007map} show how ten popular machine learning algorithms
can be written in a ``summation form'' in which parallelization is
straightforward. The authors implemented these algorithms on a MapReduce-like
framework and ran them on multicore machines. They yielded a near-linear
speedup as the number of cores was increased.

Whereas Chu et al. experimented on single multicore machines, our project
extends their ideas to a cluster of networked computers. Rather than use a
framework like Hadoop~\cite{hadoop}, which is intended for large batch
processing jobs, we have designed and implemented a lightweight framework for
low-latency tasks that requires minimal server configuration.

\section{Choosing a Language}

Our two criteria for language choice were ease of development and good support
for linear algebra operations. C++ is known to provide excellent performance,
but it is not conducive to rapid prototyping. MATLAB's licensing costs make it
infeasible for use on large clusters. We evaluated Python and R as possible
options.

The following sections compare Python and R and explain why we chose Python.

\subsection{Code Style}

R is stylistically very similar to MATLAB. Matrix and list operations are
first-class operations, and it provides built-in equivalents to most of
MATLAB's probability and statistics functions. Also, the R interpreter makes
plotting and visualizing data structures just as easy as in MATLAB.

While Python's syntax is less suited for matrix operations, the NumPy
package for Python~\cite{numpy} includes Matlib, an interface that attempts to
emulate MATLAB's syntax. It is designed specifically for MATLAB programmers and
to ease the porting of MATLAB code. Certain syntactic elements are still overly
verbose (e.g., \texttt{M.transpose()} vs \texttt{M'}) and may hinder the
readability of an algorithm.

Strictly from a syntactic and stylistic perspective, R wins on its simplicity
and similarity to MATLAB. However, Python's slight disadvantage here is far
outweighed by its relative performance, as described below.

\subsection{Performance}

Python is not a mathematical language, but it is easily extensible and provides
many high-quality mathematics packages (NumPy in particular). Though
interpreted, Python can easily call down to C and avoid dynamic-language
overhead in computationally intensive operations. For instance, NumPy's matrix
multiplication is implemented in C, yielding very good performance
without sacrificing ease of use or readability in the Python code.

We benchmarked the performance of R against Python's (using the NumPy package).
In order to avoid implementation- or algorithm-specific bias, we decided to
benchmark the common linear algebra functions, such as matrix multiplication,
that are ubiquitous in learning algorithms.

Table \ref{PythonvsR} shows the running times of Python and R on various
operations using different matrix or vector sizes, as well as the time ratio of
$Python / R$. Every test ran 10,000 operations. In terms of performance, Python
is the clear winner. It outperforms R in every case, most of the cases by an
order of magnitude. In the worst case, Python takes 82\% of the time that R
takes.

\begin{table}[h!]
  \begin{center}
    \vspace{1em}
    \textbf{Matrix Multiplication} \\
    \begin{tabular}{cccc}
      Size  & Python  &  R    & Python / R \\
      \hline
      50  & 0.1600  & 0.2208  & 0.7246 \\
      75  & 0.5800  & 0.7339  & 0.7903 \\
      100 & 1.3030  & 1.6323  & 0.7983 \\
      150 & 4.2350  & 5.2311  & 0.8096 \\
      250 & 18.9190 & 22.9759 & 0.8234 \\
    \end{tabular}

    \vspace{1em}
    \textbf{Element-Wise Matrix Multiplication} \\
    \begin{tabular}{cccc}
      Size  & Python  &  R       & Python / R \\
      \hline
      150   & 0.0350  &  0.1576  &  0.2221 \\
      225   & 0.0760  &  0.3741  &  0.2032 \\
      300   & 0.1510  &  0.6859  &  0.2201 \\
      450   & 0.9310  &  2.0938  &  0.4446 \\
      750   & 3.3010  &  5.4117  &  0.6100 \\
    \end{tabular}

    \vspace{1em}
    \textbf{Matrix Transpose} \\
    \begin{tabular}{cccc}
      Size  & Python  &  R       & Python / R \\
      \hline
      50  & 0.0010  & 0.0325 & 0.0308 \\
      75  & 0.0010  & 0.0610 & 0.0164 \\
      100 & 0.0010  & 0.1030 & 0.0097 \\
      150 & 0.0010  & 0.2196 & 0.0046 \\
      250 & 0.0010  & 0.6119 & 0.0016 \\
    \end{tabular}

    \vspace{1em}
    \textbf{Vector Inner Product} \\
    \begin{tabular}{cccc}
      Size  & Python &  R       & Python / R \\
      \hline
      2500  & 0.0040 & 0.0523 & 0.0765 \\
      3750  & 0.0060 & 0.0772 & 0.0777 \\
      5000  & 0.0070 & 0.1030 & 0.0680 \\
      7500  & 0.0100 & 0.1519 & 0.0658 \\
      12500 & 0.0160 & 0.2514 & 0.0636 \\
    \end{tabular}
  \end{center}
  \caption{Benchmarks of Python and R for linear algebra
    operations.}
  \label{PythonvsR}
\end{table}

Although na\"{i}ve Python implementations of {\it serial} machine learning
algorithms tend to be slower than their MATLAB equivalents, recent benchmarks
of the {\it parallel} sparse autoencoder show that Python's performance penalty
is not as significant in parallel execution as it is amortized over the number
of workers. Also, since we are targeting rapid prototyping, not peak production
performance, a small performance hit is acceptable.

\section{Architecture}

This section describes the architecture of the qjam framework. Subsection
\ref{Components} defines the major components of the system and how they
communicate with each other. Subsection \ref{Library} explains the programming
interface. Subsection \ref{Protocol} describes the protocol that Qjam uses to
communicate. Finally, subsection \ref{Implementation} describes details of
Qjam's Python implementation.

\subsection{Components}
\label{Components}

Qjam is a single-master distributed system made up of instances of the
following components: \\

\begin{description}
  \item[Worker] --- The Worker is a program that is copied to all of the remote
    machines during the bootstrapping process. It is responsible for waiting
    for instructions from the Master, and upon receiving work, processing that
    work and returning the result. \\

  \item[RemoteWorker] --- The RemoteWorker is a special Python class that
    communicates with the remote machines. One RemoteWorker has a single target
    machine that can be reached via ssh. There can be many RemoteWorkers with
    the same target (say, in the case where there are many cores on a machine),
    but only one target per RemoteWorker. At creation, the RemoteWorker
    bootstraps the remote machine by copying the requisite files to run the
    Worker program, via ssh. After the bootstrapping process completes, the
    RemoteWorker starts a Worker process on the remote machine and attaches to
    it. The RemoteWorker is the proxy between the Master and the Worker. \\

  \item[Master] --- The Master is a Python class that divides up work and
    assigns the work units among its pool of RemoteWorker instances. These
    RemoteWorker instances relay the work to the Worker programs running on the
    remote machines and wait for the results. \\
\end{description}

Figure \ref{diagram} shows the communication channels between components on
multiple machines.

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=3.3in]{fwk_diagram/fwk_diagram.pdf}
  \end{center}
  \caption{Master controlling four RemoteWorkers with Workers in two machines.}
  \label{diagram}
\end{figure}

\subsection{Qjam Library API}
\label{Library}

This section describes the interface exposed by Qjam and provides a simple
example of its use. The workflow of a typical distributed computation on Qjam
is divided into two phases. In the initialization phase, the client creates an
instance of the \texttt{Master} class by passing its constructor a list of
remote workers. In the execution phase, the client specifies a Python module
containing a function, \texttt{mapfunc}, to be executed on each worker along
with a dataset and a list of parameters for the computation. The framework then
breaks the dataset into smaller pieces and distributes the work to the worker
processes. The following two subsections elaborate on the details.

\subsubsection{Initialization}

At a fundamental level, the Qjam library is built on top of the
\texttt{RemoteWorker} class. An instance of \texttt{RemoteWorker} defines a
single connection to a worker node. A collection of \texttt{RemoteWorker}
objects is passed to the constructor of \texttt{Master} to define the pool of
workers available to the master.

The following code demonstrates the initialization of the worker pool and
master.

{\tt \small
\begin{verbatim}
  workers = [RemoteWorker(`corn15.stanford.edu'),
             RemoteWorker(`corn16.stanford.edu'),
             RemoteWorker(`corn17.stanford.edu')]
  master = Master(workers)
\end{verbatim}}

\subsubsection{Execution}

Once the list of \texttt{RemoteWorker}s and the \texttt{Master} have been
initialized, the client must first wrap any static data into a \texttt{DataSet}
object, and then it can issue a call to \texttt{master.run(...)} to distribute
the computation. Results of the work are obtained through the return value of
this call.

\paragraph{Creating a \texttt{DataSet} Object}
\label{DataSets}

In order for \texttt{Master} to know how to partition the task data between the
registered \texttt{RemoteWorker}s, it needs to have some notion of how the data
is structured. In order to fulfill that requirement, the client can either
resort to one of the convenience \texttt{DataSet} classes provided by Qjam, or
define a custom data class that inherits from \texttt{BaseDataSet}.

The \texttt{DataSet} classes provided by Qjam include support for Python tuples
or lists of any kind, or NumPy matrices. In the case the client wishes to
represent the data as a matrix, he can choose between
\texttt{NumpyMatrixDataSet}, which simply represents the matrix in-memory, or
\texttt{NumpyMatrixFileDataSet}, which represents the matrix as a file on disk
in the case that it is too large to fit in memory.

In the case that the client wishes to define a custom data set class that
inherits from Qjam's \texttt{BaseDataSet} class, he must implement at least
the following two member functions:
\begin{enumerate}
  \item
    \texttt{chunks()}
    Returns the number of chunks into which the internal data can be divided.
  \item
    \texttt{slice(index)}
    Returns the slice of data at the given index where index is an integer
    in [0, $chunks() - 1$].
\end{enumerate}


\paragraph{Defining a Code Module}

The code that is to be executed at each remote worker must be written by the
client in a self-contained Python module. The module must contain a function
called \texttt{mapfunc} that will be called by the framework. The function
\texttt{mapfunc} must take two arguments: The first argument are the parameters,
$\theta$, passed by the client in the call to \texttt{master.run}. $\theta$ can
be of any type and is passed, without modification, to every remote worker. The
second argument of \texttt{mapfunc} is a subset of the \texttt{DataSet} created
by the client as described in section \ref{DataSets}. Note that Qjam guarantees
that different workers will receive different, non-overlapping subset of the
data.

The client also has the option of defining a \texttt{reduce} function as part
of the same module. If the client opts out of this option, then the return
value of \texttt{mapfunc} must be of a type that defines the sum operator or
a list of types that define the sum operator. More complex return values are
possible if the client defines a custom reduce function.

A simple \texttt{mapfunc} might be defined as follows.

{\tt \small
\begin{verbatim}
  def multiply_sum(theta, dataset):
    return sum([theta * x_i for x_i in dataset])

  mapfunc = multiply_sum
\end{verbatim}}


\paragraph{Calling \texttt{master.run}}

Once the code module and the dataset object have been defined, the client can
make a call to the function \texttt{master.run} to distribute the
computation. The \texttt{master.run} function takes the client-defined code
module, the parameters, and a \texttt{DataSet} object as arguments. The return
value of \texttt{master.run} is the result of the computation.

The following simple example shows a call to \texttt{master.run}.

{\tt \small
\begin{verbatim}
  from examples import multiply_sum

  params = 42
  dataset = ListDataSet(range(1, 100))
  result = master.run(multiply_sum, params, dataset)
\end{verbatim}}


\subsection{Protocol}
\label{Protocol}

Communication between the Qjam master and each of the workers occurs via
a connection that is persistent throughout the existence of the \texttt{master}
object. This section describes the details of the communication protocol that
is used by our implementation to assign tasks to remote workers, efficiently
distribute data, and coalesce results.

The protocol implementation relies on five different types of messages. Figure
\ref{ProtocolDiagram} shows the use of those messages in a typical
communication workflow.

\begin{figure}[!h]
  \centering
  \includegraphics[scale=0.35]{protocol-diagram.pdf}
  \caption{Communication between \texttt{Master} and a single
           \texttt{RemoteWorker} during the execution of a typical Qjam
           distributed computation}
  \label{ProtocolDiagram}
\end{figure}


\subsubsection{\textsc{task} Message}

The \textsc{task} message type is sent by the master to each worker to initiate
a task. It contains an encoded\footnote{Objects are encoded using a base64
  representation of the serialized Python object.} representation of the
client's code module, a hash of the chunks that compose the worker's assigned
dataset, and an encoded representation of the client-specified parameters.


\subsubsection{\textsc{state} Message}

Upon receiving a \textsc{task} message, the worker must respond with a
\textsc{state} message. This message contains a status field that can take one
of two values: ``\texttt{running}'' or ``\texttt{blocked}''. In the case that
the status field is set to blocked, the worker must include a separate field
whose value is a list of hash values where each hash value identifies a chunk
of the dataset that the worker is missing.


\subsubsection{\textsc{refs} Message}

If the master receives a \textsc{state} message whose status is set to
``\texttt{blocked}'', then it responds with a \textsc{refs} message. This type
of message includes a list of encoded objects that correspond to the data
chunks that the worker identified as missing.


\subsubsection{\textsc{result} Message}

Finally, the \textsc{result} message is sent to the from the worker to the
master whenever it completes its task. This message contains an encoded
representation of the computation's result.


\subsubsection{\textsc{error} Message}

In the case that the worker encounters an unexpected state, it can send an
\textsc{error} reply to any message sent by the master. This message contains
a description of the error.


\subsection{Feature Highlights}
\label{Implementation}

\subsubsection{Remote Data Caching}
One important feature of Qjam is caching of data by each remote worker. The
master initially sends each worker a list of the hash values of each data chunk
that the worker will need for a given task. If a worker cannot find the data
object that corresponds to one or more hash values, it requests them from the
master and caches them. In later iterations, the master can take advantage of
data locality by assigning workers data chunks that they have seen before.

\subsubsection{Automatic Bootstrapping}
An important design goal for Qjam is to make the execution of a distributed
computation as easy as possible. To that end, our implementation strives to
minimize the amount of setup necessary on each worker machine. Qjam has the
ability to transparently bootstrap each remote worker with all of the code it
needs to communicate with the master. After initiating an SSH connection to a
worker, the master sends a source code of the worker protocol implementation and
remotely executes it on the worker node. This allows any computer with Python
and an SSH server to serve as a remote worker---no manual setup required.


\section{Evaluation}

We benchmarked the framework running various algorithms with multiple workers.

\subsection{L-BFGS Sparse Autoencoder}

We benchmarked qjam using a sparse autoencoder with L-BFGS~\cite{lbfgs}. A
sparse autoencoder is an unsupervised learning algorithm that automatically
learns features from unlabeled data. It is implemented as a neural network with
one hidden layer (parameters) that adjusts its weight values at each iteration
over the training set. L-BFGS is a limited-memory, quasi-Newton optimization
method for unconstrained optimization.

We benchmarked the running time of the sparse autoencoder using a parallelized
cost function (with L-BFGS optimizing it). We tested a regular single-core
implementation against 2, 4, 8, and 16 workers over four multicore machines. We
tested with three datasets (of 1,000, 10,000, and 100,000 patches each). Table
\ref{table_iteration_timea} summarizes per-iteration results, while Table
\ref{table_total_time} is the sum of all iterations plus the master's setup
overhead.

\begin{table}[htbp]
  \small
  \begin{center}
    \begin{tabular}{crrr}
    workers  & \multicolumn{1}{c}{1k} & \multicolumn{1}{c}{10k}
             & \multicolumn{1}{c}{100k} \\
      \hline
    1  & 0.1458 (1.0x) & 0.7310 (1.0x) & 10.0282 (1.0x) \\
    2  & 0.1752 (0.8x) & 0.3321 (2.2x) & 4.6782  (2.1x) \\
    4  & 0.2634 (0.5x) & 0.3360 (2.2x) & 2.4858  (4.0x) \\
    8  & 0.5339 (0.3x) & 0.5251 (1.4x) & 1.8046  (5.6x) \\
    16 & 0.9969 (0.2x) & 1.0186 (0.7x) & 1.4376  (6.9x) \\
    \end{tabular}
  \end{center}
  \caption{Iteration Mean Time (seconds)}
  \label{table_iteration_timea}
\end{table}
\begin{table}[htbp]
  \small
  \begin{center}
    \begin{tabular}{crrr}
    workers  & \multicolumn{1}{c}{1k} & \multicolumn{1}{c}{10k}
             & \multicolumn{1}{c}{100k} \\
      \hline
    1  &  76  (1.0x) & 370 (1.0x) & 5030 (1.0x) \\
    2  &  92  (0.8x) & 170 (2.2x) & 2350 (2.1x) \\
    4  & 137  (0.5x) & 173 (2.1x) & 1253 (4.0x) \\
    8  & 275  (0.3x) & 270 (1.4x) & 914  (5.5x) \\
    16 & 544  (0.1x) & 529 (0.7x) & 703  (7.2x) \\
    \end{tabular}
  \end{center}
  \caption{Total Running Time (seconds)}
  \label{table_total_time}
\end{table}

\begin{figure}[ht!]
  \centering
  \subfigure{
    \label{fig:graphs-a}
    \includegraphics[width=1.6in]{graphs/total_running_time_speedup.pdf}
  }
  \subfigure{
    \includegraphics[width=1.6in]{graphs/per_iteration_vs_total_speedup.pdf}
    \label{fig:graphs-b}
  }
  \caption{a) Total Running Time Speedup. b) Per-Iteration and Total Speedups
  (100k patches). Per-Iteration times reflect only client code, whereas Total times
  incorporate the master's coordination overhead.}
  \label{fig:graphs}
\end{figure}

For the large job (100k), qjam performs better than the single-core every
time, as seen in Figure \ref{fig:graphs-a}. The running times show a significant
speedup when using qjam with multiple workers. In particular, the 16 worker
trial saw a speedup of over 7 times the single-core's running time. Comparing
the speedups observed per-iteration against the total running time of this trial
(Figure \ref{fig:graphs-b}) reveals that the master overhead is very small,
yielding no significant slowdown.

The non-intensive job (10k) saw a small increase in performance, but not
significantly. For the smallest, trivial job (1k), the overhead of coordinating
many workers with very little to compute drove the performance below that of the
single-core's implementation, just as expected.

A particular number of workers seems to be
suited for a particular job size: for the 10k patches trials, the best run was
that with 2 workers. The others performed worse, though still most performed
better than the single core. For the largest job, though the 16 worker runtime
was the lowest, the savings from 8 workers to 16 were proportionally small,
requiring twice the number of workers for 1x more. This further confirms that
in order to minimize the overhead of distributing the job, the number of
workers should be picked according to the job size. Further research should
explore various job sizes with different worker pools.

\section{Future Work}

The next logical step in the development of Qjam is running more benchmarks with
significantly larger datasets and more iterations. It is important to observe
Qjam's performance and reliability under high stress. Moreover, having more data
about Qjam's performance would more clearly reveal whatever bottlenecks remain.


With regard to features, another important step is to achieve feature parity
with other, more general parallel frameworks (e.g. MapReduce). Handling worker
failures, anticipating stragglers, and using a smarter job scheduling algorithm
will likely yield performance improvements, particularly when running on larger
or heterogeneous clusters than those we tested on.

We currently use SSH and JSON to transfer data and messages. Using a more
efficient protocol and data encoding will improve performance and
reliability. We noticed that SSH occasionally dropped connections and
implemented a workaround to automatically reconnect upon failure; this, however,
remains the biggest source of instability on Qjam.

Finally, aside from implementation related improvements, we will also
improve usability. As a start, we can offer a wider range of convenience DataSet
subclasses---beyond those that encapsulate matrices and lists (e.g.,
ImageDataSet, AudioDataSet).


\section{Conclusion}

We have presented a framework that greatly simplifies the rapid prototyping of
distributed machine learning algorithms. Qjam provides an abstraction of the
complexities associated with building an entire distributed system just to run
a task on multiple computers in parallel. As a result, it is now possible
rapidly prototype and execute distributed computations without having to
explicitly manage communication overhead and the distribution of code and data
to remote workers.

Moreover, Qjam offers satisfactory performance. On a computation that takes a
mean of 10 seconds to complete on a single machine, we observed 4x speed-up on
4 workers and 7x speedup on 16 workers.

\section{Acknowledgements}
We would like to recognize the assistance of those in our CS 229 class research
group: Prof. Andrew Ng, Adam Coates, Bobby Prochnow, Milinda Lakkam, Sisi
Sarkizova, Raghav Pasari, and Abhik Lahiri.


\bibliographystyle{IEEEbib}
\bibliography{report}

\end{document}
