\documentclass[%
  %draft,
  %submission,
  %compressed,
  final,
  %
  %technote,
  %internal,
  %submitted,
  %inpress,
  %reprint,
  %
  %titlepage,
  notitlepage,
  %anonymous,
  narroweqnarray,
  inline,
  %twoside,
]{ieee}

\usepackage{ieeefig, url, enumerate}

\begin{document}

\title{Parallelizing Machine Learning Algorithms}

\author[SHORT NAMES]{
  \begin{tabular*}{0.75\textwidth}{@{\extracolsep{\fill}}cccc}
    Juan Batiz-Benet & Quinn Slack & Matt Sparks & Ali Yahya \\
    \multicolumn{4}{c}{
      \normalsize
      \url{{jbenet,sqs,msparks,alive}@cs.stanford.edu}}
  \end{tabular*}
}

\maketitle

\begin{abstract}
Qjam is a framework for the rapid prototyping of parallel machine
learning algorithms on clusters.
\end{abstract}

\section{Introduction}

Implementing machine learning algorithms consists of performing computationally
intensive operations on large data sets. As these data sets grow in size and
algorithms grow in complexity, it becomes necessary to spread the work among
multiple computers and multiple cores. Fortunately, many machine learning
algorithms are easy to parallelize in theory. However, the fixed cost of
creating a distributed system that organizes and manages the work is an
obstacle to parallelizing existing algorithms and prototyping new ones. We
present Qjam, a Python library that transparently parallelizes certain machine
learning algorithms that adhere to a constrained MapReduce model of
computation.

We would like to recognize the assistance of those in our CS 229 class research
group: Prof. Andrew Ng, Adam Coates, Bobby Prochnow, Milinda Lakkam, Sisi
Sarkizova, Raghav Pasari, and Abhik Lahiri.

\section{Previous Work}

Significant research has been done in the area of distributed data
processing. We targeted algorithms written in the MapReduce programming
model~\cite{mapreduce}, as constrained by the ``summation form'' introduced by
Chu et al.~\cite{chu2007map}. This form separates the parallel work from the
serial work in machine learning algorithms, and the authors showed that many
common algorithms can be written in this way. They implemented these algorithms
on a MapReduce-like framework and ran them on multicore machines, which yielded
a near-linear speedup as the number of cores was increased.

Whereas Chu et al. experimented on single multicore machines, our project
extends their ideas to a cluster of networked computers. Rather than use a
framework like Hadoop, which is intended for large batch processing jobs, we
have designed and implemented a lightweight framework for low-latency tasks
that requires minimal server configuration.

\section{Choosing a Language}

Our two criteria for language choice were ease of development and good support
for linear algebra operations. C++ is known to provide excellent performance,
but it is not conducive to rapid prototyping. MATLAB's licensing costs make it
infeasible for use on large clusters. We evaluated Python and R as possible
options.

The following sections compare Python and R and explain why we chose Python.

\subsection{Code Style}

R is stylistically very similar to MATLAB. Matrix and list operations are
first-class operations, and it provides built-in equivalents to most of
MATLAB's probability and statistics functions. Also, the R interpreter makes
it just as easy as MATLAB to plot and visualize data structures.

While Python's syntax is less suited for matrix operations, the NumPy
package~\cite{numpy} for Python includes Matlib, an interface that attempts to
emulate MATLAB's syntax. It is designed specifically for MATLAB programmers and
to ease the porting of MATLAB code. Certain syntactic elements are still overly
verbose (e.g., \texttt{M.transpose()} vs \texttt{M'}) and may hinder the
readability of an algorithm.

Strictly from a syntactic and stylistic perspective, R wins on its simplicity
and resemblance to MATLAB. However, Python's slight disadvantage here is far
outweighed by the considerations presented below.

\subsection{Functionality}

We had two options when implementing our framework. We could have written the
internals in something like C++ and make external calls to Python or R, or we
could have implemented the internals in the scripting language itself. Each
approach has its advantages, but under time pressure, the latter approach was
preferable. Because Python is general-purpose scripting language, it is by far
the better choice with respect to functionality. R is primarily a language for
statistical computing and therefore lacks the library support necessary for the
implementation of a distributed system.

\subsection{Performance}

Python is not a mathematical language, but it is easily extensible and provides
many high-quality mathematics packages (NumPy in particular). Though
interpreted, Python can easily call down to C and avoid dynamic-language
overhead in computationally intensive operations. For instance, NumPy's matrix
multiplication is implemented in C, yielding very good performance
without sacrificing ease of use or readability in the Python code.

We benchmarked the performance of R against Python's (using the NumPy package).
In order to avoid implementation- or algorithm-specific bias, we decided to
benchmark the common linear algebra functions, such as matrix multiplication,
that are ubiquitous in learning algorithms.

Table \ref{PythonvsR} shows the running times of Python and R on various
operations using different matrix or vector sizes, as well as the time ratio of
$Python / R$. Every test ran 10,000 operations.

\begin{table}[h!]
  \begin{center}
    \vspace{1em}
    \textbf{Matrix Addition} \\
    \begin{tabular}{cccc}
      Size  & Python  &  R      & Python / R \\
      \hline
      50  & 0.0060 & 0.0159 & 0.3774 \\
      75  & 0.0110 & 0.0302 & 0.3642 \\
      100 & 0.0170 & 0.0519 & 0.3276 \\
      150 & 0.0350 & 0.1161 & 0.3015 \\
      250 & 0.0950 & 0.3396 & 0.2797 \\
    \end{tabular}

    \vspace{1em}
    \textbf{Matrix Multiplication} \\
    \begin{tabular}{cccc}
      Size  & Python  &  R    & Python / R \\
      \hline
      50  & 0.1600  & 0.2208  & 0.7246 \\
      75  & 0.5800  & 0.7339  & 0.7903 \\
      100 & 1.3030  & 1.6323  & 0.7983 \\
      150 & 4.2350  & 5.2311  & 0.8096 \\
      250 & 18.9190 & 22.9759 & 0.8234 \\
    \end{tabular}

    \vspace{1em}
    \textbf{Element-Wise Matrix Multiplication} \\
    \begin{tabular}{cccc}
      Size  & Python  &  R       & Python / R \\
      \hline
      150   & 0.0350  &  0.1576  &  0.2221 \\
      225   & 0.0760  &  0.3741  &  0.2032 \\
      300   & 0.1510  &  0.6859  &  0.2201 \\
      450   & 0.9310  &  2.0938  &  0.4446 \\
      750   & 3.3010  &  5.4117  &  0.6100 \\
    \end{tabular}

    \vspace{1em}
    \textbf{Matrix Transpose} \\
    \begin{tabular}{cccc}
      Size  & Python  &  R       & Python / R \\
      \hline
      50  & 0.0010  & 0.0325 & 0.0308 \\
      75  & 0.0010  & 0.0610 & 0.0164 \\
      100 & 0.0010  & 0.1030 & 0.0097 \\
      150 & 0.0010  & 0.2196 & 0.0046 \\
      250 & 0.0010  & 0.6119 & 0.0016 \\
    \end{tabular}

    \vspace{1em}
    \textbf{Vector inner product} \\
    \begin{tabular}{cccc}
      Size  & Python &  R       & Python / R \\
      \hline
      2500  & 0.0040 & 0.0523 & 0.0765 \\
      3750  & 0.0060 & 0.0772 & 0.0777 \\
      5000  & 0.0070 & 0.1030 & 0.0680 \\
      7500  & 0.0100 & 0.1519 & 0.0658 \\
      12500 & 0.0160 & 0.2514 & 0.0636 \\
    \end{tabular}
  \end{center}
  \caption{Benchmarks of Python and R for linear algebra
    operations.}
  \label{PythonvsR}
\end{table}

In terms of performance, Python is the clear winner. It outperforms R in every
case, most of the cases by an order of magnitude. In the worst case, Python
takes 82\% of the time that R takes.

Although naive Python implementations of {\it serial} machine learning algorithms
tend to be slower than their MATLAB equivalents, recent benchmarks of the {\it
  parallel} sparse autoencoder show that Python's performance penalty is not as
significant in parallel execution. Also, since we are targeting rapid
prototyping, not peak production performance, a small performance hit is
acceptable.

\section{Architecture}

This section describes the architecture of the qjam framework. Subsection
\ref{Components} defines the major components of the system and how they
communicate. Subsection \ref{Library} explains the programming
interface. Subsection \ref{DataSet} explains our \texttt{DataSet} objects and
how they are used to divide data among workers. Finally, Subsection
\ref{Storage} covers how the worker obtains and stores data.

\subsection{Components}
\label{Components}

Qjam is a single-master distributed system made up of instances of the
following components: \\

\begin{description}
  \item[Worker] --- The Worker is a program that is copied to all of the remote
    machines during the bootstrapping process. It is responsible for waiting
    for instructions from the Master, and upon receiving work, processing that
    work and returning the result. \\

  \item[RemoteWorker] --- The RemoteWorker is a special Python class that
    communicates with the remote machines. One RemoteWorker has a single target
    machine that can be reached via ssh. There can be many RemoteWorkers with
    the same target (say, in the case where there are many cores on a machine),
    but only one target per RemoteWorker. At creation, the RemoteWorker
    bootstraps the remote machine by copying the requisite files to run the
    Worker program, via ssh. After the bootstrapping process completes, the
    RemoteWorker starts a Worker process on the remote machine and attaches to
    it. The RemoteWorker is the proxy between the Master and the Worker. \\

  \item[Master] --- The Master is a Python class that divides up work and
    assigns the work units among its pool of RemoteWorker instances. These
    RemoteWorker instances relay the work to the Worker programs running on the
    remote machines and wait for the results. \\
\end{description}

Fig. \ref{diagram} shows the communication channels between components on
multiple machines.

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=3.3in]{fwk_diagram/fwk_diagram.pdf}
  \end{center}
  \caption{Master controlling four RemoteWorkers with Workers in two machines.}
  \label{diagram}
\end{figure}

\subsection{Qjam Library API}
\label{Library}

% TODO(ms): Write this section. (Someone else, not me)

TODO. Use Listings package.

\subsection{DataSet Objects}
\label{DataSet}

% TODO(ms): Write this section. (Someone else, not me)

\subsection{Serialization and Worker-Local Storage}
\label{Storage}

% TODO(ms): 
% Serialization may not be that important, but just in case:
%   - how we do serialization of the python code
%     package up the self-contained module code, ship it over the wire, load it
%   - how we serialize data
%     for now, json/base64. Mostly for simplicity.

Each worker stores its slices of the data set in memory, not on disk. When it
is given a task and references to data on which to perform that task, it
requests all missing data slices from the master. This will generally only
happen on the first iteration, after which the master will heed data locality
in assigning tasks. The data slices are then deserialized on the worker and
passed as arguments to the {\tt map} function.

Using Python's reflection facilities, we also send the code for the Qjam
library and the {\tt map} function to the workers for each job. This frees
developers from having to manually update their code, and the Qjam code, on
each worker. (In fact, any computer with Python and an SSH server can become a
Qjam worker, with no additional manual setup.)

Our implementation of worker-local storage is similar to the SlaveRefs in Adam
Coates' Matlab parallel framework, but we are able to use Python's superior
syntax to hide the implementation details of references to data on
workers. Also, our system as described will only send portions of the data to
each worker, which entails lower data transmission overhead relative to
distributing the entire data set to each worker.

\section{Evaluation}

We benchmarked the framework running various algorithms with multiple workers.

\subsection{L-BFGS Sparse Auto-Encoder}

We benchmarked qjam using a sparse autoencoder with L-BFGS~\cite{lbfgs}. A
sparse autoencoder is an unsupervised learning algorithm that automatically
learns features from unlabeled data. It is implemented as a neural network with
one hidden layer (parameters) that adjusts its weight values at each iteration
over the training set. L-BFGS is a limited-memory, quasi-Newton optimization
method for unconstrained optimization.

We benchmarked the running time of the sparse autoencoder using a parallelized
cost function (with L-BFGS optimizing it). We tested a regular single-core
implementation against 2, 4, 8, and 16 workers over four multicore machines. We
tested with three datasets (of sizes 1000, 10,000, and 100,000). We obtained
the following results:

% TODO(ms): use table environment here with a label.
\begin{center}

\textbf{Iteration Mean Time (seconds)} \\
\begin{tabular}{cccc}
workers  & 1k  &  10k   & 100k   \\
  \hline
1  & 0.1458 & 0.7310 & 10.0282 \\
2  & 0.1752 & 0.3321 & 4.6782 \\
4  & 0.2634 & 0.3360 & 2.4858 \\
8  & 0.5339 & 0.5251 & 1.8046 \\
16 & 0.9969 & 1.0186 & 1.5862 \\
\end{tabular}

\vspace{1em}
\textbf{Total Running Time (seconds)} \\
\begin{tabular}{cccc}
workers  & 1k  &  10k   & 100k   \\
  \hline
1  & 76 (1.00) & 370 (1.00) & 5030 (1.00) \\
2  & 92 (1.21) & 170 (0.46) & 2350 (0.47) \\
4  & 137 (1.80) & 173 (0.46) & 1253 (0.25) \\
8  & 275 (3.61) & 270 (0.73) & 914 (0.18) \\
16 & 544 (7.51) & 529 (1.43) & 703 (0.14) \\
\end{tabular}
\end{center}

% TODO(ms): A graph or two of this data would be nice. It would be best to use
% the same style (i.e., axes) as in the Chu et al. paper.

The running times show a remarkable speedup when using qjam with multiple
workers. For our longest run, we saw a drop to under 14\% of the single-core's
running time. For big jobs (10,000, 100,000), qjam performs very well, beating
the single-core every time. For non-intensive jobs (1000), the overhead of many
workers can drive the performance beyond that of the single-core's
implementation.

This leads to another observation: a particular number of workers seems to be
suited for a particular job size: for the 10,000 patches runs, the best run was
that with 2 workers. Though the others still performed better then the single
core, they performed worse. For the largest job, though the 16 worker runtime
was the lowest, the savings from 8 workers to 16 were small. This further
confirms that the number of workers should be picked according to the job size,
to minimize the overhead of distributing the job.

\section{Future Work}

% TODO(ms): Add mentions about the following:
%
% Possible future improvements:
%   binary formats (BSON) for sending larger data on wire
%   more profiling for optimization
%   doing stuff over ssh has some nice properties, but it is also unnecessary
%     setup overhead. Might want to consider using it only for bootstrapping
%     but not communication.
%   splitting data into smaller chunks (like the code currently almost does)
%   handling worker failure (reroute work to other workers)
%   handling stragglers (duplicate work to other workers if one is slow)
%     ^^^ the above two are common in mapreduce
%   more datasets (maybe one that references files on a shared filesystem)
%   persistent worker caches -- I implemented this recently, but a problem in
%     the dataset library stopped us from using it for the experiments.
%     Persistent here means that the ref cache lives across restarts of the
%     workers, so starting a new job on old data is fast.

\section{Conclusion}

\bibliographystyle{IEEEbib}
\bibliography{report}

\end{document}
